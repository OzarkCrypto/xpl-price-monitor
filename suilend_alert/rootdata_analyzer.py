#!/usr/bin/env python3
"""
Rootdata ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ê¸°
ì‹¤ì œ hot index ë°ì´í„°ê°€ ì–´ë””ì— ìˆëŠ”ì§€ íŒŒì•…í•˜ê³  íŒŒì‹± ë¡œì§ì„ ê°œë°œ
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime

class RootdataAnalyzer:
    def __init__(self):
        """Rootdata ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.url = "https://www.rootdata.com/"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def analyze_page_structure(self):
        """í˜ì´ì§€ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            print("ğŸ” Rootdata ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ ì¤‘...")
            
            response = self.session.get(self.url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            print(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {soup.title.string if soup.title else 'N/A'}")
            print(f"ğŸ“Š í˜ì´ì§€ í¬ê¸°: {len(response.content):,} bytes")
            
            # 1. ëª¨ë“  í…Œì´ë¸” ë¶„ì„
            self.analyze_tables(soup)
            
            # 2. ëª¨ë“  divì™€ í´ë˜ìŠ¤ ë¶„ì„
            self.analyze_divs_and_classes(soup)
            
            # 3. JavaScript ë°ì´í„° ë¶„ì„
            self.analyze_javascript_data(soup)
            
            # 4. íŠ¹ì • í‚¤ì›Œë“œ ê²€ìƒ‰
            self.search_for_keywords(soup)
            
            # 5. í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥
            self.save_page_source(response.text)
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def analyze_tables(self, soup):
        """í…Œì´ë¸” êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("\nğŸ“‹ í…Œì´ë¸” ë¶„ì„:")
        tables = soup.find_all('table')
        print(f"ì´ {len(tables)}ê°œì˜ í…Œì´ë¸” ë°œê²¬")
        
        for i, table in enumerate(tables):
            print(f"\ní…Œì´ë¸” {i+1}:")
            rows = table.find_all('tr')
            print(f"  í–‰ ìˆ˜: {len(rows)}")
            
            if rows:
                # ì²« ë²ˆì§¸ í–‰ì˜ í—¤ë” ë¶„ì„
                headers = rows[0].find_all(['th', 'td'])
                header_texts = [h.get_text(strip=True) for h in headers]
                print(f"  í—¤ë”: {header_texts}")
                
                # ë°ì´í„° í–‰ ìƒ˜í”Œ
                if len(rows) > 1:
                    data_row = rows[1].find_all('td')
                    data_texts = [d.get_text(strip=True) for d in data_row]
                    print(f"  ë°ì´í„° ìƒ˜í”Œ: {data_texts}")
    
    def analyze_divs_and_classes(self, soup):
        """divì™€ í´ë˜ìŠ¤ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("\nğŸ—ï¸ div ë° í´ë˜ìŠ¤ ë¶„ì„:")
        
        # hot ê´€ë ¨ í´ë˜ìŠ¤ ì°¾ê¸°
        hot_elements = soup.find_all(class_=lambda x: x and 'hot' in x.lower())
        print(f"hot ê´€ë ¨ í´ë˜ìŠ¤: {len(hot_elements)}ê°œ")
        
        for i, element in enumerate(hot_elements[:5]):  # ì²˜ìŒ 5ê°œë§Œ
            print(f"  {i+1}. í´ë˜ìŠ¤: {element.get('class')}")
            print(f"     í…ìŠ¤íŠ¸: {element.get_text(strip=True)[:100]}...")
        
        # index ê´€ë ¨ í´ë˜ìŠ¤ ì°¾ê¸°
        index_elements = soup.find_all(class_=lambda x: x and 'index' in x.lower())
        print(f"index ê´€ë ¨ í´ë˜ìŠ¤: {len(index_elements)}ê°œ")
        
        # ranking ê´€ë ¨ í´ë˜ìŠ¤ ì°¾ê¸°
        ranking_elements = soup.find_all(class_=lambda x: x and 'rank' in x.lower())
        print(f"ranking ê´€ë ¨ í´ë˜ìŠ¤: {len(ranking_elements)}ê°œ")
    
    def analyze_javascript_data(self, soup):
        """JavaScript ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("\nğŸ’» JavaScript ë°ì´í„° ë¶„ì„:")
        
        scripts = soup.find_all('script')
        print(f"ì´ {len(scripts)}ê°œì˜ script íƒœê·¸ ë°œê²¬")
        
        for i, script in enumerate(scripts):
            if script.string:
                script_content = script.string
                
                # hot index ê´€ë ¨ ë°ì´í„° ì°¾ê¸°
                if 'hot' in script_content.lower() and 'index' in script_content.lower():
                    print(f"\n  Script {i+1}ì—ì„œ hot index ê´€ë ¨ ë°ì´í„° ë°œê²¬:")
                    print(f"    ë‚´ìš©: {script_content[:200]}...")
                
                # JSON ë°ì´í„° ì°¾ê¸°
                json_matches = re.findall(r'\{[^{}]*"hot"[^{}]*\}', script_content)
                if json_matches:
                    print(f"\n  Script {i+1}ì—ì„œ JSON ë°ì´í„° ë°œê²¬:")
                    for match in json_matches[:3]:
                        print(f"    {match}")
                
                # ë°°ì—´ ë°ì´í„° ì°¾ê¸°
                array_matches = re.findall(r'\[[^\[\]]*"hot"[^\[\]]*\]', script_content)
                if array_matches:
                    print(f"\n  Script {i+1}ì—ì„œ ë°°ì—´ ë°ì´í„° ë°œê²¬:")
                    for match in array_matches[:3]:
                        print(f"    {match}")
    
    def search_for_keywords(self, soup):
        """íŠ¹ì • í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        print("\nğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰:")
        
        keywords = ['hot', 'index', 'ranking', 'trend', 'popular', 'score']
        
        for keyword in keywords:
            elements = soup.find_all(string=re.compile(keyword, re.IGNORECASE))
            if elements:
                print(f"  '{keyword}' í‚¤ì›Œë“œ: {len(elements)}ê°œ ë°œê²¬")
                for element in elements[:3]:
                    parent = element.parent
                    if parent:
                        print(f"    {parent.get_text(strip=True)[:100]}...")
    
    def save_page_source(self, html_content):
        """í˜ì´ì§€ ì†ŒìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"rootdata_source_{timestamp}.html"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"\nğŸ’¾ í˜ì´ì§€ ì†ŒìŠ¤ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def test_parsing_methods(self):
        """ë‹¤ì–‘í•œ íŒŒì‹± ë°©ë²•ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print("\nğŸ§ª íŒŒì‹± ë°©ë²• í…ŒìŠ¤íŠ¸:")
        
        try:
            response = self.session.get(self.url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë°©ë²• 1: ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ hot index íŒ¨í„´ ì°¾ê¸°
            print("\n1. í…ìŠ¤íŠ¸ íŒ¨í„´ ê²€ìƒ‰:")
            all_text = soup.get_text()
            hot_patterns = re.findall(r'hot\s*index[:\s]*([\d,]+)', all_text, re.IGNORECASE)
            if hot_patterns:
                print(f"   ë°œê²¬ëœ hot index ê°’ë“¤: {hot_patterns[:10]}")
            
            # ë°©ë²• 2: íŠ¹ì • êµ¬ì¡°ì—ì„œ ë°ì´í„° ì°¾ê¸°
            print("\n2. êµ¬ì¡°ì  ë°ì´í„° ê²€ìƒ‰:")
            data_elements = soup.find_all(['div', 'span', 'td'], class_=re.compile(r'data|value|score'))
            print(f"   ë°ì´í„° ê´€ë ¨ ìš”ì†Œ: {len(data_elements)}ê°œ")
            
            # ë°©ë²• 3: API ì—”ë“œí¬ì¸íŠ¸ ì°¾ê¸°
            print("\n3. API ì—”ë“œí¬ì¸íŠ¸ ê²€ìƒ‰:")
            api_patterns = re.findall(r'https?://[^\s"\'<>]+api[^\s"\'<>]*', all_text)
            if api_patterns:
                print(f"   ë°œê²¬ëœ API URLë“¤: {api_patterns[:5]}")
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ” Rootdata ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ê¸°")
    print("=" * 50)
    
    analyzer = RootdataAnalyzer()
    
    try:
        # ê¸°ë³¸ êµ¬ì¡° ë¶„ì„
        analyzer.analyze_page_structure()
        
        # íŒŒì‹± ë°©ë²• í…ŒìŠ¤íŠ¸
        analyzer.test_parsing_methods()
        
        print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. ì €ì¥ëœ HTML íŒŒì¼ì„ í™•ì¸í•˜ì—¬ ì‹¤ì œ ë°ì´í„° êµ¬ì¡° íŒŒì•…")
        print("2. ë°œê²¬ëœ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ íŒŒì‹± ë¡œì§ ê°œë°œ")
        print("3. rootdata_hot_index_monitor.pyì˜ parse_hot_index ë©”ì„œë“œ ì—…ë°ì´íŠ¸")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main() 