#!/usr/bin/env python3
"""
PYTH Insights ê³ ê¸‰ í¼ë¸”ë¦¬ì…” ì •ë³´ ê°€ì ¸ì˜¤ê¸°
React ì•±ì˜ ì‹¤ì œ ë°ì´í„° APIë¥¼ ì°¾ì•„ì„œ í¼ë¸”ë¦¬ì…” ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
"""

import requests
import json
import time
import re
from typing import Dict, List, Optional

class PythInsightsAdvancedFetcher:
    def __init__(self):
        self.base_url = "https://insights.pyth.network"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://insights.pyth.network/',
            'Origin': 'https://insights.pyth.network',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'Connection': 'keep-alive'
        })
    
    def get_webpage_content(self, page: int = 1) -> str:
        """ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        url = f"{self.base_url}/publishers?page={page}"
        
        try:
            response = self.session.get(url, timeout=15)
            if response.status_code == 200:
                return response.text
            else:
                print(f"âŒ í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status_code}")
                return ""
        except requests.exceptions.RequestException as e:
            print(f"ğŸ’¥ í˜ì´ì§€ {page} ì—°ê²° ì‹¤íŒ¨: {e}")
            return ""
    
    def find_data_in_html(self, html_content: str) -> List[Dict]:
        """HTMLì—ì„œ í¼ë¸”ë¦¬ì…” ë°ì´í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        publishers = []
        
        # 1. JSON ë°ì´í„°ê°€ í¬í•¨ëœ ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ì°¾ê¸°
        script_patterns = [
            r'<script[^>]*>.*?window\.__INITIAL_STATE__\s*=\s*({.*?});.*?</script>',
            r'<script[^>]*>.*?window\.__NEXT_DATA__\s*=\s*({.*?});.*?</script>',
            r'<script[^>]*>.*?window\.__PRELOADED_STATE__\s*=\s*({.*?});.*?</script>',
            r'<script[^>]*>.*?publishers\s*:\s*(\[.*?\])',
            r'<script[^>]*>.*?data\s*:\s*(\[.*?\])',
            r'<script[^>]*>.*?({.*?"publishers".*?})',
            r'<script[^>]*>.*?({.*?"ranking".*?})',
            r'<script[^>]*>.*?({.*?"active".*?})'
        ]
        
        for pattern in script_patterns:
            matches = re.findall(pattern, html_content, re.DOTALL | re.IGNORECASE)
            for match in matches:
                try:
                    data = json.loads(match)
                    if self._is_publisher_data(data):
                        publishers.extend(self._extract_publishers_from_data(data))
                except json.JSONDecodeError:
                    pass
        
        # 2. ì¸ë¼ì¸ JSON ë°ì´í„° ì°¾ê¸°
        json_patterns = [
            r'publishers\s*:\s*(\[.*?\])',
            r'data\s*:\s*(\[.*?\])',
            r'ranking\s*:\s*(\[.*?\])',
            r'active\s*:\s*(\[.*?\])'
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, html_content, re.DOTALL | re.IGNORECASE)
            for match in matches:
                try:
                    data = json.loads(match)
                    if isinstance(data, list) and data:
                        publishers.extend(self._extract_publishers_from_data(data))
                except json.JSONDecodeError:
                    pass
        
        # 3. í…Œì´ë¸” ë°ì´í„° íŒŒì‹±
        table_publishers = self._parse_table_data(html_content)
        publishers.extend(table_publishers)
        
        return publishers
    
    def _parse_table_data(self, html_content: str) -> List[Dict]:
        """HTML í…Œì´ë¸”ì—ì„œ í¼ë¸”ë¦¬ì…” ë°ì´í„°ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        publishers = []
        
        # í…Œì´ë¸” í–‰ ì°¾ê¸°
        table_row_pattern = r'<tr[^>]*>.*?</tr>'
        rows = re.findall(table_row_pattern, html_content, re.DOTALL | re.IGNORECASE)
        
        for row in rows:
            # í…Œì´ë¸” ì…€ ë°ì´í„° ì¶”ì¶œ
            cell_pattern = r'<td[^>]*>(.*?)</td>'
            cells = re.findall(cell_pattern, row, re.DOTALL | re.IGNORECASE)
            
            if len(cells) >= 5:  # ìµœì†Œ 5ê°œ ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•¨
                try:
                    # HTML íƒœê·¸ ì œê±°
                    clean_cells = []
                    for cell in cells:
                        clean_cell = re.sub(r'<[^>]+>', '', cell).strip()
                        clean_cells.append(clean_cell)
                    
                    # í¼ë¸”ë¦¬ì…” ì •ë³´ êµ¬ì„±
                    if clean_cells[0] and clean_cells[0] != 'Loading':
                        publisher = {
                            'ranking': self._extract_number(clean_cells[0]),
                            'name': clean_cells[1] if len(clean_cells) > 1 else 'Unknown',
                            'permissioned_feeds': self._extract_number(clean_cells[2]) if len(clean_cells) > 2 else 0,
                            'active_feeds': self._extract_number(clean_cells[3]) if len(clean_cells) > 3 else 0,
                            'average_score': self._extract_float(clean_cells[4]) if len(clean_cells) > 4 else 0.0
                        }
                        publishers.append(publisher)
                        
                except Exception as e:
                    continue
        
        return publishers
    
    def _extract_number(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        numbers = re.findall(r'\d+', text)
        return int(numbers[0]) if numbers else 0
    
    def _extract_float(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ì—ì„œ ì†Œìˆ˜ì  ìˆ«ìë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        numbers = re.findall(r'\d+\.?\d*', text)
        return float(numbers[0]) if numbers else 0.0
    
    def _is_publisher_data(self, data) -> bool:
        """ë°ì´í„°ê°€ í¼ë¸”ë¦¬ì…” ì •ë³´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        if isinstance(data, dict):
            publisher_keywords = ['publishers', 'publisher', 'ranking', 'active', 'permissioned', 'score']
            for key in data.keys():
                if any(keyword in str(key).lower() for keyword in publisher_keywords):
                    return True
        elif isinstance(data, list):
            if data and isinstance(data[0], dict):
                first_item = data[0]
                publisher_fields = ['name', 'id', 'ranking', 'active', 'permissioned', 'score']
                if any(field in first_item for field in publisher_fields):
                    return True
        return False
    
    def _extract_publishers_from_data(self, data) -> List[Dict]:
        """ë°ì´í„°ì—ì„œ í¼ë¸”ë¦¬ì…” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        publishers = []
        
        if isinstance(data, dict):
            # publishers í‚¤ê°€ ìˆëŠ” ê²½ìš°
            if 'publishers' in data:
                publishers.extend(self._extract_publishers_from_data(data['publishers']))
            # data í‚¤ê°€ ìˆëŠ” ê²½ìš°
            elif 'data' in data:
                publishers.extend(self._extract_publishers_from_data(data['data']))
            # ì§ì ‘ í¼ë¸”ë¦¬ì…” ì •ë³´ì¸ ê²½ìš°
            elif self._is_publisher_data(data):
                publishers.append(data)
                
        elif isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    if self._is_publisher_data(item):
                        publishers.append(item)
                    else:
                        # ì¤‘ì²©ëœ ë°ì´í„° ê²€ìƒ‰
                        nested = self._extract_publishers_from_data(item)
                        publishers.extend(nested)
        
        return publishers
    
    def try_graphql_endpoint(self) -> List[Dict]:
        """GraphQL ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‹œë„í•©ë‹ˆë‹¤."""
        print("ğŸ” GraphQL ì—”ë“œí¬ì¸íŠ¸ ì‹œë„ ì¤‘...")
        
        graphql_endpoints = [
            "/graphql",
            "/api/graphql",
            "/v1/graphql",
            "/v2/graphql"
        ]
        
        for endpoint in graphql_endpoints:
            url = f"{self.base_url}{endpoint}"
            
            # GraphQL ì¿¼ë¦¬
            query = """
            query GetPublishers {
                publishers {
                    id
                    name
                    ranking
                    activeFeeds
                    permissionedFeeds
                    averageScore
                    cluster
                    status
                }
            }
            """
            
            try:
                response = self.session.post(url, json={'query': query}, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    if 'data' in data and 'publishers' in data['data']:
                        print(f"  âœ… GraphQL ë°ì´í„° ë°œê²¬: {endpoint}")
                        return data['data']['publishers']
            except Exception as e:
                print(f"  âŒ GraphQL ì‹¤íŒ¨: {endpoint} - {e}")
        
        return []
    
    def try_rest_api_with_params(self) -> List[Dict]:
        """íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” REST APIë¥¼ ì‹œë„í•©ë‹ˆë‹¤."""
        print("ğŸ” REST API íŒŒë¼ë¯¸í„° ì‹œë„ ì¤‘...")
        
        # ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•© ì‹œë„
        params_combinations = [
            {'page': 1, 'limit': 100},
            {'page': 1, 'size': 100},
            {'page': 1, 'per_page': 100},
            {'offset': 0, 'limit': 100},
            {'start': 0, 'count': 100},
            {'page': 1},
            {'limit': 100},
            {}
        ]
        
        # ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸ë“¤
        endpoints = [
            "/api/publishers",
            "/api/v1/publishers",
            "/api/v2/publishers",
            "/api/data/publishers",
            "/api/insights/publishers",
            "/api/stats/publishers"
        ]
        
        for endpoint in endpoints:
            for params in params_combinations:
                url = f"{self.base_url}{endpoint}"
                
                try:
                    response = self.session.get(url, params=params, timeout=10)
                    if response.status_code == 200:
                        try:
                            data = response.json()
                            if self._is_publisher_data(data):
                                print(f"  âœ… REST API ë°ì´í„° ë°œê²¬: {endpoint} with {params}")
                                return self._extract_publishers_from_data(data)
                        except json.JSONDecodeError:
                            pass
                except Exception as e:
                    continue
        
        return []
    
    def fetch_all_publishers(self) -> Dict:
        """ëª¨ë“  ë°©ë²•ì„ ì‹œë„í•˜ì—¬ í¼ë¸”ë¦¬ì…” ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        print("ğŸš€ PYTH Insightsì—ì„œ í¼ë¸”ë¦¬ì…” ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹œì‘...")
        
        all_publishers = []
        
        # 1. GraphQL ì‹œë„
        graphql_publishers = self.try_graphql_endpoint()
        if graphql_publishers:
            all_publishers.extend(graphql_publishers)
            print(f"âœ… GraphQLì—ì„œ {len(graphql_publishers)}ê°œ í¼ë¸”ë¦¬ì…” ë°œê²¬")
        
        # 2. REST API ì‹œë„
        rest_publishers = self.try_rest_api_with_params()
        if rest_publishers:
            all_publishers.extend(rest_publishers)
            print(f"âœ… REST APIì—ì„œ {len(rest_publishers)}ê°œ í¼ë¸”ë¦¬ì…” ë°œê²¬")
        
        # 3. ì›¹í˜ì´ì§€ íŒŒì‹± ì‹œë„
        if not all_publishers:
            print("ğŸŒ ì›¹í˜ì´ì§€ íŒŒì‹± ì‹œë„ ì¤‘...")
            for page in range(1, 5):  # 4í˜ì´ì§€ê¹Œì§€ ì‹œë„
                html_content = self.get_webpage_content(page)
                if html_content:
                    page_publishers = self.find_data_in_html(html_content)
                    if page_publishers:
                        all_publishers.extend(page_publishers)
                        print(f"  âœ… í˜ì´ì§€ {page}ì—ì„œ {len(page_publishers)}ê°œ í¼ë¸”ë¦¬ì…” ë°œê²¬")
                    else:
                        print(f"  âš ï¸  í˜ì´ì§€ {page}ì—ì„œ í¼ë¸”ë¦¬ì…” ì •ë³´ ì—†ìŒ")
                        if page > 1:  # ì²« í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ ì¤‘ë‹¨
                            break
                else:
                    break
                
                time.sleep(1)
        
        # ì¤‘ë³µ ì œê±°
        unique_publishers = []
        seen_ids = set()
        
        for pub in all_publishers:
            pub_id = pub.get('id', pub.get('name', ''))
            if pub_id not in seen_ids:
                seen_ids.add(pub_id)
                unique_publishers.append(pub)
        
        return {
            'publishers': unique_publishers,
            'total_count': len(unique_publishers),
            'source': 'multiple_sources',
            'raw_data': all_publishers
        }
    
    def save_results(self, data: Dict, filename: str = "pyth_insights_publishers_advanced.json"):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # CSVë¡œë„ ì €ì¥
        if data.get('publishers'):
            csv_filename = filename.replace('.json', '.csv')
            import csv
            
            with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['Ranking', 'Name', 'ID', 'Active Feeds', 'Permissioned Feeds', 'Average Score', 'Cluster', 'Status'])
                
                for pub in data['publishers']:
                    writer.writerow([
                        pub.get('ranking', ''),
                        pub.get('name', ''),
                        pub.get('id', ''),
                        pub.get('active_feeds', pub.get('activeFeeds', '')),
                        pub.get('permissioned_feeds', pub.get('permissionedFeeds', '')),
                        pub.get('average_score', pub.get('averageScore', '')),
                        pub.get('cluster', ''),
                        pub.get('status', '')
                    ])
            
            print(f"ğŸ“Š CSV íŒŒì¼ë„ {csv_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def print_summary(self, data: Dict):
        """ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print("\n=== PYTH Insights í¼ë¸”ë¦¬ì…” ì •ë³´ ìš”ì•½ ===")
        
        if data.get('publishers'):
            publishers = data['publishers']
            total_count = data.get('total_count', len(publishers))
            source = data.get('source', 'unknown')
            
            print(f"ğŸ“Š ì´ í¼ë¸”ë¦¬ì…” ìˆ˜: {total_count}")
            print(f"ğŸ”— ë°ì´í„° ì†ŒìŠ¤: {source}")
            print(f"ğŸ“… ê²€ìƒ‰ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            
            print(f"\n=== ìƒìœ„ 10ê°œ í¼ë¸”ë¦¬ì…” ===")
            sorted_publishers = sorted(publishers, key=lambda x: x.get('ranking', 999))
            
            for i, pub in enumerate(sorted_publishers[:10]):
                print(f"{i+1:2d}. {pub.get('name', 'Unknown')} (Rank: {pub.get('ranking', 'N/A')})")
                print(f"    Active: {pub.get('active_feeds', pub.get('activeFeeds', 0))}, Permissioned: {pub.get('permissioned_feeds', pub.get('permissionedFeeds', 0))}")
                print(f"    Score: {pub.get('average_score', pub.get('averageScore', 0)):.2f}, Cluster: {pub.get('cluster', 'Unknown')}")
                print()
        else:
            print("âŒ í¼ë¸”ë¦¬ì…” ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def main():
    fetcher = PythInsightsAdvancedFetcher()
    
    # ëª¨ë“  ë°©ë²•ìœ¼ë¡œ í¼ë¸”ë¦¬ì…” ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    publishers_data = fetcher.fetch_all_publishers()
    
    # ê²°ê³¼ ì¶œë ¥
    fetcher.print_summary(publishers_data)
    
    # ê²°ê³¼ ì €ì¥
    if publishers_data.get('publishers'):
        fetcher.save_results(publishers_data)
    else:
        print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 